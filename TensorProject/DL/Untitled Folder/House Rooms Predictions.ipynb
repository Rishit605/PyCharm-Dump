{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "12faaf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\users\\pc\\anaconda3\\envs\\tensor\\lib\\site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c253db8",
   "metadata": {},
   "source": [
    "# Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fdb32f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading house-rooms-image-dataset.zip to C:\\Users\\pc\\PycharmProjects\\TensorProject\\DL\\Untitled Folder\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/116M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/116M [00:00<00:31, 3.85MB/s]\n",
      "  3%|3         | 4.00M/116M [00:00<00:09, 12.6MB/s]\n",
      "  5%|5         | 6.00M/116M [00:00<00:13, 8.50MB/s]\n",
      "  9%|8         | 10.0M/116M [00:00<00:08, 12.7MB/s]\n",
      " 10%|#         | 12.0M/116M [00:01<00:08, 12.7MB/s]\n",
      " 12%|#2        | 14.0M/116M [00:01<00:07, 13.9MB/s]\n",
      " 14%|#3        | 16.0M/116M [00:01<00:07, 14.7MB/s]\n",
      " 16%|#6        | 19.0M/116M [00:01<00:06, 16.9MB/s]\n",
      " 18%|#8        | 21.0M/116M [00:01<00:06, 15.5MB/s]\n",
      " 20%|#9        | 23.0M/116M [00:01<00:06, 15.6MB/s]\n",
      " 21%|##1       | 25.0M/116M [00:01<00:06, 15.4MB/s]\n",
      " 23%|##3       | 27.0M/116M [00:02<00:05, 16.2MB/s]\n",
      " 25%|##4       | 29.0M/116M [00:02<00:06, 14.7MB/s]\n",
      " 27%|##6       | 31.0M/116M [00:02<00:05, 15.6MB/s]\n",
      " 28%|##8       | 33.0M/116M [00:02<00:05, 15.3MB/s]\n",
      " 30%|###       | 35.0M/116M [00:02<00:05, 15.1MB/s]\n",
      " 32%|###1      | 37.0M/116M [00:02<00:05, 14.8MB/s]\n",
      " 34%|###3      | 39.0M/116M [00:02<00:05, 14.7MB/s]\n",
      " 35%|###5      | 41.0M/116M [00:03<00:05, 14.0MB/s]\n",
      " 37%|###6      | 43.0M/116M [00:03<00:05, 13.6MB/s]\n",
      " 40%|###9      | 46.0M/116M [00:03<00:04, 15.8MB/s]\n",
      " 41%|####1     | 48.0M/116M [00:03<00:04, 15.4MB/s]\n",
      " 43%|####2     | 50.0M/116M [00:03<00:04, 14.5MB/s]\n",
      " 46%|####5     | 53.0M/116M [00:03<00:04, 14.6MB/s]\n",
      " 47%|####7     | 55.0M/116M [00:04<00:04, 15.2MB/s]\n",
      " 49%|####8     | 57.0M/116M [00:04<00:03, 15.6MB/s]\n",
      " 51%|#####     | 59.0M/116M [00:04<00:03, 15.7MB/s]\n",
      " 52%|#####2    | 61.0M/116M [00:04<00:03, 14.8MB/s]\n",
      " 54%|#####4    | 63.0M/116M [00:04<00:03, 15.3MB/s]\n",
      " 56%|#####5    | 65.0M/116M [00:04<00:03, 15.1MB/s]\n",
      " 58%|#####7    | 67.0M/116M [00:04<00:03, 15.2MB/s]\n",
      " 59%|#####9    | 69.0M/116M [00:05<00:03, 14.7MB/s]\n",
      " 61%|######    | 71.0M/116M [00:05<00:02, 16.0MB/s]\n",
      " 63%|######2   | 73.0M/116M [00:05<00:04, 9.13MB/s]\n",
      " 65%|######5   | 76.0M/116M [00:05<00:03, 12.0MB/s]\n",
      " 67%|######7   | 78.0M/116M [00:05<00:03, 12.3MB/s]\n",
      " 69%|######8   | 80.0M/116M [00:05<00:02, 13.7MB/s]\n",
      " 70%|#######   | 82.0M/116M [00:06<00:03, 9.04MB/s]\n",
      " 72%|#######2  | 84.0M/116M [00:06<00:03, 10.7MB/s]\n",
      " 74%|#######3  | 86.0M/116M [00:06<00:03, 10.5MB/s]\n",
      " 76%|#######5  | 88.0M/116M [00:07<00:03, 8.34MB/s]\n",
      " 77%|#######7  | 90.0M/116M [00:08<00:06, 4.30MB/s]\n",
      " 79%|#######9  | 92.0M/116M [00:08<00:04, 5.44MB/s]\n",
      " 81%|########  | 94.0M/116M [00:08<00:03, 6.80MB/s]\n",
      " 82%|########2 | 96.0M/116M [00:08<00:02, 8.34MB/s]\n",
      " 84%|########4 | 98.0M/116M [00:08<00:02, 7.53MB/s]\n",
      " 86%|########5 | 100M/116M [00:08<00:01, 9.24MB/s] \n",
      " 88%|########7 | 102M/116M [00:09<00:01, 10.7MB/s]\n",
      " 89%|########9 | 104M/116M [00:09<00:01, 12.4MB/s]\n",
      " 91%|#########1| 106M/116M [00:09<00:01, 8.23MB/s]\n",
      " 94%|#########3| 109M/116M [00:09<00:00, 10.7MB/s]\n",
      " 95%|#########5| 111M/116M [00:09<00:00, 11.7MB/s]\n",
      " 97%|#########7| 113M/116M [00:10<00:00, 8.03MB/s]\n",
      "100%|#########9| 116M/116M [00:10<00:00, 10.6MB/s]\n",
      "100%|##########| 116M/116M [00:10<00:00, 11.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d robinreni/house-rooms-image-dataset --unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1743b9",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cdbc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# SciKit Learn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imutils import paths\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd06bd1",
   "metadata": {},
   "source": [
    "## Assiging Variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c386b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_df = \"./House_Room_Dataset/Bedroom\"\n",
    "\n",
    "Tr = \"training\"\n",
    "Val = \"evaluation\"\n",
    "Ts = \"testing\"\n",
    "\n",
    "base_path = \"dataset\"\n",
    "batch_size = 32\n",
    "Classes = [\"Modern\", \"Old\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e1e4c",
   "metadata": {},
   "source": [
    "## Base-Line Configuration for graph Plotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5481978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(hist, metric):\n",
    "    if metric == 'auc':\n",
    "        plt.plot(hist.history[\"auc\"])\n",
    "        plt.plot(hist.history[\"val_auc\"])\n",
    "    else:\n",
    "        plt.plot(hist.history[\"loss\"])\n",
    "        plt.plot(hist.history[\"val_loss\"])\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.title('model {}'.format(metric))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(\"{}\".format(metric))\n",
    "    plt.legend([\"train\", \"validation\"], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eddae13",
   "metadata": {},
   "source": [
    "## Loading Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "344a0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading labels from the txt file\n",
    "with open(\"C:/Users/pc/Desktop/AI and ML/convolution/datasets/House rooms interiors/Labels/labels.txt\", 'r') as f:\n",
    "      manual_labels = f.read()\n",
    "        \n",
    "# Extracting individual labels into a list\n",
    "labels = [i for i in manual_labels]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc3ce71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['O', 'M'])\n",
      "dict_values([271, 180])\n"
     ]
    }
   ],
   "source": [
    "# For checking the equillibrium of the dataset\n",
    "from collections import Counter\n",
    "print(Counter(labels).keys()) \n",
    "print(Counter(labels).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674e577",
   "metadata": {},
   "source": [
    "## Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40286d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bed_1.jpg', 'bed_2.jpg', 'bed_3.jpg', 'bed_4.jpg', 'bed_8.jpg']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting files in the order they appear\n",
    "files = os.listdir(ip_df)\n",
    "files.sort(key=lambda f: int(f.split('_')[1].split('.')[0]))\n",
    "\n",
    "# checking to see the correct file order\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66342588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344, 344, 61, 61, 46, 46)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the Dataset[Test Set]\n",
    "X_train, X_test, y_train, y_test = train_test_split(files[:len(labels)],labels, stratify=labels, train_size = 0.9)\n",
    "\n",
    "# Splitting the dataset[Validation Set]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, train_size=0.85)\n",
    "\n",
    "# Checking the size of train, test, eval\n",
    "len(X_train), len(y_train), len(X_val), len(y_val),  len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00511898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating over images in sub folder: 100%|█████████████████████████████████████████| 344/344 [00:00<00:00, 1619.33it/s]\n",
      "Iterating over images in sub folder: 100%|███████████████████████████████████████████| 46/46 [00:00<00:00, 1072.63it/s]\n",
      "Iterating over images in sub folder: 100%|███████████████████████████████████████████| 61/61 [00:00<00:00, 1422.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Building the dataset properly - \n",
    "splits = [(X_train, y_train), (X_test, y_test), (X_val, y_val)]\n",
    "dirnames = ['training', 'testing', 'validation']\n",
    "\n",
    "for i, (data,label) in enumerate(splits):\n",
    "    outside_dir=dirnames[i]\n",
    "\n",
    "    for j in tqdm(range(0, len(label)), desc=\"Iterating over images in sub folder\"):\n",
    "        dir = label[j]\n",
    "        \n",
    "        # construct the path to the sub-directory\n",
    "        dirPath = os.path.join(base_path, outside_dir, dir)\n",
    "        \n",
    "        # if the output directory does not exist, create it\n",
    "        if not os.path.exists(dirPath):\n",
    "            os.makedirs(dirPath)\n",
    "            \n",
    "            \n",
    "        # copy the img to this new directory\n",
    "        src_img = os.path.join(ip_df, data[j])\n",
    "        shutil.copy(src_img, dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "afbfe459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 46 46\n"
     ]
    }
   ],
   "source": [
    "trainPath = os.path.sep.join([base_path, Tr])\n",
    "valPath = os.path.sep.join([base_path, Val])\n",
    "testPath = os.path.sep.join([base_path, Ts])\n",
    "\n",
    "totalTrain = len(list(paths.list_images(trainPath)))\n",
    "totalVal = len(list(paths.list_images(valPath)))\n",
    "totalTest = len(list(paths.list_images(testPath)))\n",
    "\n",
    "print(totalTrain, totalTest, totalVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3850158",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c5ed992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    zoom_range=[0.5, 1.0],\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    brightness_range=[0.2, 1.0]\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8822cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default for all the above parameters is 0, \n",
    "# meaning we are applying no augmentation to val set\n",
    "# which is exactly what we need because val set should be treated like test set.\n",
    "valAug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "daa2cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testAug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86852d9e",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acf8299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 344 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create batches whilst creating augmented images on the fly\n",
    "\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "    directory=trainPath,\n",
    "    target_size=(224,224),\n",
    "    save_to_dir='./dataset/augmented/train',\n",
    "    save_prefix='train',\n",
    "    shuffle=True # data will be shuffled between epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2bedaa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valGen = valAug.flow_from_directory(\n",
    "    directory=valPath,\n",
    "    target_size=(224,224),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6bf1b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "testGen = testAug.flow_from_directory(\n",
    "    directory=testPath,\n",
    "    target_size=(224,224),\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1c679",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "> ## Feature Extraction Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b2b23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = EfficientNetB0(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False, # make sure top layer is not included\n",
    "            input_tensor=Input(shape=(224, 224, 3)),\n",
    "            pooling=\"avg\"\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "636e24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the weights\n",
    "for layer in baseModel.layers:\n",
    "      layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7501d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a new classifier on top (Functional Keras Model)x = baseModel.output\n",
    "x = baseModel.output\n",
    "\n",
    "Layer_1 = BatchNormalization()(x)\n",
    "Layer_2 = Dropout(0.5)(Layer_1)\n",
    "output_layer = Dense(len(Classes), activation=\"softmax\")(Layer_2)\n",
    "\n",
    "model = Model(inputs = baseModel.input, outputs = output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ba4f35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to create the classifier on top of basemodelmodel = tf.keras.Sequential()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(baseModel)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(Classes), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7f1b0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.AUC()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1aef7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing early stopping\n",
    "es = EarlyStopping(\n",
    "     monitor='val_loss',  #metric to monitor\n",
    "     mode='min',  # whether to min or max the metric monitored\n",
    "     patience=10, # epochs to wait before declaring stopped training\n",
    "     verbose=1  # output epoch when training was stopped\n",
    "     )\n",
    "\n",
    "# implementing model checkpoint\n",
    "mc = ModelCheckpoint(\n",
    "      'feature_extraction.h5',\n",
    "       monitor='val_loss',\n",
    "       mode='min',\n",
    "       verbose=1, # display epoch+accuracy everytime model is saved\n",
    "       save_best_only=True\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3a584254",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/augmented/train\\\\train_207_334489.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [122]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainGen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalGen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotalTrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1143\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m cluster_coordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1138\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1141\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1142\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1159\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1400\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1399\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1155\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1152\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1154\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1169\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:934\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_sequence \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enqueuer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKerasSequenceAdapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shuffle is handed in the _make_callable override.\u001b[39;49;00m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:811\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28msuper\u001b[39m(GeneratorDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# Since we have to know the dtype of the python generator when we build the\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m peek, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_peek_and_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m peek \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_batch(peek)\n\u001b[0;32m    813\u001b[0m peek \u001b[38;5;241m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:945\u001b[0m, in \u001b[0;36mKerasSequenceAdapter._peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_peek_and_restore\u001b[39m(x):\n\u001b[1;32m--> 945\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py:65\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_index_array()\n\u001b[0;32m     63\u001b[0m index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_array[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m idx:\n\u001b[0;32m     64\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py:250\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    244\u001b[0m         img \u001b[38;5;241m=\u001b[39m array_to_img(batch_x[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    245\u001b[0m         fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{prefix}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{index}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{hash}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{format}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    246\u001b[0m             prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_prefix,\n\u001b[0;32m    247\u001b[0m             index\u001b[38;5;241m=\u001b[39mj,\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;28mhash\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e7\u001b[39m),\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_format)\n\u001b[1;32m--> 250\u001b[0m         \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# build batch of labels\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\PIL\\Image.py:2297\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2295\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2297\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2300\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/augmented/train\\\\train_207_334489.png'"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "hist = model.fit(\n",
    "    x=trainGen,\n",
    "    epochs=25,\n",
    "    verbose=2,\n",
    "    validation_data=valGen,\n",
    "    steps_per_epoch=totalTrain // batch_size,\n",
    "    callbacks=[es, mc]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415552b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2cbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
